[project]
name = "homelab-ai-in-docker"
version = "0.1.0"
description = "REST API service wrapping common AI capabilities for homelab developers"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "crawl4ai>=0.7.0",
    "pydantic>=2.0.0",
    "sentence-transformers>=3.0.0",
    "torch>=2.7.0",
    "torchvision>=0.20.0",
    "pillow>=10.0.0",
    "transformers==4.46.3",
    "tokenizers==0.20.3",
    "sse-starlette>=2.0.0",
    "psutil>=6.0.0",
    "gputil>=1.4.0",
    "accelerate>=0.20.0",
    "bitsandbytes>=0.41.0; platform_system=='Linux'",
    "paddlepaddle>=3.0.0",
    "paddleocr>=3.0.0",
    "mineru-vl-utils>=0.1.0",
    "einops>=0.7.0",
    "docling-core>=2.0.0",
    "addict>=2.4.0",
    "matplotlib>=3.7.0",
    "easydict>=1.9",
    "librosa>=0.10.0",
    "pyannote.audio>=3.1.0",
    "omegaconf>=2.0.0",
]

[project.optional-dependencies]
# GPU acceleration (NVIDIA CUDA 12.4, PyTorch 2.7)
# Uses pre-built flash-attn wheels to avoid compilation
# Docker setup assumes GPU availability and installs this by default
gpu = [
    "torch==2.7.0",
    "torchvision==0.20.0",
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu124torch2.7-cp313-cp313-linux_x86_64.whl; platform_system=='Linux'",
]

[tool.setuptools]
packages = ["src"]

[tool.setuptools.package-data]
"src.api.models" = ["*.json"]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]
