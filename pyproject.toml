[project]
name = "homelab-ai-in-docker"
version = "0.1.0"
description = "REST API service wrapping common AI capabilities for homelab developers"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "crawl4ai>=0.7.0",
    "pydantic>=2.0.0",
    "sentence-transformers>=3.0.0",
    "torch>=2.0.0",
    "torchvision>=0.15.0",
    "pillow>=10.0.0",
    "transformers==4.46.3",
    "tokenizers==0.20.3",
    "sse-starlette>=2.0.0",
    "psutil>=6.0.0",
    "gputil>=1.4.0",
    "accelerate>=0.20.0",
    "bitsandbytes>=0.41.0; platform_system=='Linux'",
    "paddlepaddle>=3.0.0",
    "paddleocr>=3.0.0",
    "mineru-vl-utils>=0.1.0",
    "einops>=0.7.0",
    "docling-core>=2.0.0",
    "addict>=2.4.0",
    "matplotlib>=3.7.0",
    "easydict>=1.9",
    "librosa>=0.10.0",
    "pyannote.audio>=3.1.0",
    "omegaconf>=2.0.0",
]

[project.optional-dependencies]
# Optional GPU acceleration for DeepSeek-OCR and Granite Docling
# Note:
# - flash-attn requires a CUDA-enabled PyTorch and typically a build toolchain
# - We keep this in an optional extra and also attempt a best-effort install in Docker entrypoint
gpu = [
    "flash-attn==2.7.3; platform_system=='Linux'",
]

[tool.setuptools]
packages = ["src"]

[tool.setuptools.package-data]
"src.api.models" = ["*.json"]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]
