[project]
name = "homelab-ai-in-docker"
version = "0.1.0"
description = "REST API service wrapping common AI capabilities for homelab developers"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "crawl4ai>=0.7.0",
    "playwright>=1.43.0",
    "pydantic>=2.0.0",
    "sentence-transformers>=3.0.0",
    "torch>=2.7.0",
    "torchvision>=0.20.0",
    "pillow>=10.0.0",
    "transformers>=4.51.0",
    "tokenizers>=0.20.3",
    "sse-starlette>=2.0.0",
    "psutil>=6.0.0",
    "nvidia-ml-py>=12.0.0",
    "accelerate>=0.20.0",
    "bitsandbytes>=0.41.0; platform_system=='Linux'",
    "paddlepaddle>=3.0.0",
    "paddleocr[doc-parser]>=3.0.0",
    "paddlex[ocr]>=3.0.0",
    "mineru-vl-utils>=0.1.0",
    "einops>=0.7.0",
    "docling-core>=2.0.0",
    "addict>=2.4.0",
    "matplotlib>=3.7.0",
    "easydict>=1.9",
    "librosa>=0.10.0",
    "pyannote.audio>=3.1.0",
    "omegaconf>=2.0.0",
]

[project.optional-dependencies]
# GPU acceleration (NVIDIA CUDA 12.6, PyTorch 2.7.0)
# Uses pre-built flash-attn wheels to avoid compilation
# Docker setup assumes GPU availability and installs this by default
gpu = [
    "flash-attn>=2.8.3; platform_system=='Linux'",
]

[tool.setuptools]
packages = ["src"]

[tool.setuptools.package-data]
"src.api.models" = ["*.json"]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]

# PyTorch CUDA 12.6 index configuration for uv
[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true

[tool.uv.sources]
torch = { index = "pytorch-cu126" }
torchvision = { index = "pytorch-cu126" }
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu126torch2.7-cp313-cp313-linux_x86_64.whl" }
# Custom safetensors build with paddle framework support for PaddleOCR-VL
safetensors = { url = "https://paddle-whl.bj.bcebos.com/nightly/cu126/safetensors/safetensors-0.6.2.dev0-cp38-abi3-linux_x86_64.whl" }
