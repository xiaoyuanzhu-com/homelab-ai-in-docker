[project]
name = "transformers-worker"
version = "0.1.0"
description = "Default HuggingFace/PyTorch worker environment for embeddings, captioning, text generation"
requires-python = ">=3.13"
dependencies = [
    # Core ML stack
    "torch==2.8.0",
    "torchvision==0.23.0",
    "torchaudio==2.8.0",
    "transformers>=4.51.0",
    "sentence-transformers>=3.0.0",
    "tokenizers>=0.20.3",
    "accelerate>=0.20.0",
    "safetensors>=0.4.0",
    # Quantization (Linux only)
    "bitsandbytes>=0.41.0; platform_system=='Linux'",
    # Image handling
    "pillow>=10.0.0",
    "pillow-heif>=1.0.0",
    # Utilities
    "einops>=0.7.0",
    # MinerU OCR
    "mineru-vl-utils[transformers]>=0.1.17",
    # Worker HTTP server
    "fastapi>=0.115.0",
    "uvicorn>=0.32.0",
    # CUDA runtime libs (Linux only)
    "nvidia-cuda-runtime-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cuda-nvrtc-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cuda-cupti-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cublas-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cufft-cu12>=11.3,<11.4; platform_system=='Linux'",
    "nvidia-nvjitlink-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cudnn-cu12==9.10.2.21; platform_system=='Linux'",
]

# Note: flash-attn requires specific pre-built wheels for each Python/CUDA/torch combo
# and will be installed separately if needed

# PyTorch CUDA 12.6 index
[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true

[tool.uv.sources]
torch = { index = "pytorch-cu126" }
torchvision = { index = "pytorch-cu126" }
torchaudio = { index = "pytorch-cu126" }
