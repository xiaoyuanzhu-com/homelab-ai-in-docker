[project]
name = "transformers-worker"
version = "0.1.0"
description = "Default HuggingFace/PyTorch worker environment for embeddings, captioning, text generation"
requires-python = ">=3.13"
dependencies = [
    # Core ML stack
    "torch==2.8.0",
    "torchvision==0.23.0",
    "torchaudio==2.8.0",
    "transformers>=4.51.0,<4.57.2",  # 4.57.2+ breaks PaddleOCR-VL (check_model_inputs signature change)
    "sentence-transformers>=3.0.0",
    "tokenizers>=0.20.3",
    "accelerate>=0.20.0",
    "safetensors>=0.4.0",
    # Quantization (Linux only)
    "bitsandbytes>=0.41.0; platform_system=='Linux'",
    # Image handling
    "pillow>=10.0.0",
    "pillow-heif>=1.0.0",
    # Utilities
    "einops>=0.7.0",
    # Required for PaddleOCR-VL tokenizer
    "sentencepiece>=0.2.0",
    "protobuf>=4.25.0",
    # MinerU OCR
    "mineru-vl-utils[transformers]>=0.1.17",
    # Segment Anything (SAM3)
    "sam3==0.1.2",
    # Worker HTTP server
    "fastapi>=0.115.0",
    "uvicorn>=0.32.0",
    # CUDA runtime libs (Linux only)
    "nvidia-cuda-runtime-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cuda-nvrtc-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cuda-cupti-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cublas-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cufft-cu12>=11.3,<11.4; platform_system=='Linux'",
    "nvidia-nvjitlink-cu12>=12.6,<12.7; platform_system=='Linux'",
    "nvidia-cudnn-cu12==9.10.2.21; platform_system=='Linux'",
    # Flash-Attention for 12x memory reduction on PaddleOCR-VL (Linux only, requires GPU)
    "flash-attn>=2.8.0; platform_system=='Linux'",
]

[tool.uv]
package = false

# PyTorch CUDA 12.6 index
[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true

[tool.uv.sources]
torch = { index = "pytorch-cu126" }
torchvision = { index = "pytorch-cu126" }
torchaudio = { index = "pytorch-cu126" }
# Flash-Attention pre-built wheel (Python 3.13 + PyTorch 2.8 + CUDA 12.6)
# Source: https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/tag/v0.4.12
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3+cu126torch2.8-cp313-cp313-linux_x86_64.whl" }
